<!DOCTYPE html>
<html>
<head>
    <title>PressReader - The Straits Times: 2020-11-13 - Is AI fi&#xAD;nally clos&#xAD;ing in on hu&#xAD;man in&#xAD;tel&#xAD;li&#xAD;gence?</title>
    <meta name="description" content="GPT-3 has been hailed as an ar&#xAD;ti&#xAD;fi&#xAD;cial in&#xAD;tel&#xAD;li&#xAD;gence break&#xAD;through. A look at its ca&#xAD;pa&#xAD;bil&#xAD;i&#xAD;ties and lim&#xAD;i&#xAD;ta&#xAD;tions.">
    <meta content="magazines, newspapers, digital news, reading, news, breaking news, newspaper online" name="keywords">
    <meta name="Robots" content="NOARCHIVE,NOODP">
    <meta charset="UTF-8">
    <meta http-equiv="Content-type" content="text/html; charset=utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    
    <link rel="canonical" href="/singapore/the-straits-times/20201113/281878710903552" />


    <style>
        li {
            margin: 1em 0;
        }
        img {
            max-width: 100%;
        }
        p, h1, h2, h3 {
            max-width: 100%;
        }
    </style>
</head>
<body>
    <div>
        

<article>
    <h1>Is AI fi&#xAD;nally clos&#xAD;ing in on hu&#xAD;man in&#xAD;tel&#xAD;li&#xAD;gence?</h1>
    <h2>GPT-3 has been hailed as an ar&#xAD;ti&#xAD;fi&#xAD;cial in&#xAD;tel&#xAD;li&#xAD;gence break&#xAD;through. A look at its ca&#xAD;pa&#xAD;bil&#xAD;i&#xAD;ties and lim&#xAD;i&#xAD;ta&#xAD;tions.</h2>
    <section>
        <a href="/singapore/the-straits-times/textview" title="The Straits Times">The Straits Times</a>
        - <a href="/singapore/the-straits-times/20201113/textview" title="The Straits Times - 2020-11-13"><time>2020-11-13</time></a>
        - <span>OPINION</span>
        - <span role="byline">John Thorn&#xAD;hill John Thorn&#xAD;hill is the Fi&#xAD;nan&#xAD;cial Times&#x2019; in&#xAD;no&#xAD;va&#xAD;tion ed&#xAD;i&#xAD;tor</span>
    </section>

    <p>It can churn out e-mails, com­puter code, In­ter­net ads, plot lines for video games, guitar riffs and sug­ges­tions for Hal­loween cos­tumes. It can write dis­arm­ingly plau­si­ble po­ems in the spirit of Emily Dick­in­son, prose in the style of Ernest Hem­ing­way and even, if so in­structed, an imag­i­nary con­ver­sa­tion be­tween Dick­in­son and Hem­ing­way.</p>
    <p>The pos­si­bil­i­ties seem al­most end­less. So when I had the op­por­tu­nity to in­ter­act with GPT-3, a new lan­guage-gen­er­a­tion model that has caused a sen­sa­tion in the ar­ti­fi­cial in­tel­li­gence (AI) world over the past few months, I en­listed its help in draft­ing my­self a new bi­og­ra­phy, in­fused with the spirit of hero Luke Sky­walker.</p>
    <p>Trained on pretty much all the text ac­ces­si­ble on the In­ter­net, GPT-3 is re­mark­able for its speed, scale and ver­sa­til­ity. This is what it in­stan­ta­neously pro­duced: “John Thorn­hill was al­ready a dec­o­rated war hero when he joined the Rebel Al­liance. He was ex­pe­ri­enced with a blaster and in­tu­itive when it came to the art of war, but his true skill lay with his squadron. Ris­ing to com­mand a squadron of his own in the Re­bel­lion, John would lead his team through nu­mer­ous bat­tles, mak­ing him a stead­fast leader with the spirit of a true Rebel.” Time to up­date my busi­ness cards.</p>
    <p>Thanks to re­cent ad­vances in ma­chine learn­ing, lan­guage gen­er­a­tion sys­tems are be­com­ing in­creas­ingly com­mon­place. Nar­ra­tive Sci­ence’s Quill can in­gest data on the per­for­mance of a port­fo­lio of stocks and write sum­maries for in­vestors, for ex­am­ple. But for sheer adapt­abil­ity none can match GPT-3, un­veiled in May by</p>
    <p>OpenAI, a San Fran­cisco-based re­search com­pany.</p>
    <p>At an es­ti­mated cost of</p>
    <p>US$12 mil­lion (S$16 mil­lion), the model con­tains 175 bil­lion lan­guage pa­ram­e­ters, 100 times more than the pre­vi­ous pro­to­type. It is, to adapt a phrase of the pi­o­neer­ing Bri­tish com­puter sci­en­tist Alan Tur­ing, the most im­pres­sive “im­i­ta­tion” ma­chine yet built.</p>
    <p>Tur­ing was one of the first peo­ple to imag­ine how the world would be trans­formed by ma­chines that could think. In his 1950 pa­per, Com­put­ing Ma­chin­ery And In­tel­li­gence, he ex­plained that com­put­ers might one day be­come so good at im­per­son­at­ing hu­mans that it would be im­pos­si­ble to dis­tin­guish them from flesh-and-blood be­ings.</p>
    <p>“We may hope that ma­chines will even­tu­ally com­pete with men in all purely in­tel­lec­tual fields,” Tur­ing wrote.</p>
    <p>AR­TI­FI­CIAL GEN­ERAL IN­TEL­LI­GENCE</p>
    <p>Such univer­sal com­put­ing ma­chines would be able to win what he called the “im­i­ta­tion game” by per­suad­ing a per­son in an elec­tronic di­a­logue that they were in­ter­act­ing with an­other hu­man be­ing, al­though some now ar­gue that this so-called Tur­ing test may be more of a re­flec­tion on hu­man gulli­bil­ity than true ma­chine in­tel­li­gence.</p>
    <p>Seventy years on, we have moved into a ma­chine-en­abled world that would stretch even Tur­ing’s imag­i­na­tion. As a re­sult of new soft­ware tech­niques, such as neu­ral net­works and deep learn­ing, com­puter sci­en­tists have be­come far bet­ter at in­struct­ing ma­chines to play the im­i­ta­tion game.</p>
    <p>Some of those who have al­ready ex­per­i­mented with GPT-3 say it is ex­hibit­ing glim­mer­ings of real in­tel­li­gence, mark­ing a sig­nif­i­cant step to­wards the ul­ti­mate end­point of AI: ar­ti­fi­cial gen­eral in­tel­li­gence (AGI), when elec­tronic in­tel­li­gence matches the hu­man kind across al­most ev­ery in­tel­lec­tual do­main.</p>
    <p>Oth­ers dis­miss this as non­sense, point­ing to GPT-3’s laugh­able flaws and sug­gest­ing we are still sev­eral con­cep­tual break­throughs away from the cre­ation of any such su­per­in­tel­li­gence.</p>
    <p>Mr Sam Alt­man, the dead­pan 35-year-old chief ex­ec­u­tive of OpenAI who is one of the high­est-pro­file fig­ures in Sil­i­con Val­ley, says there is a rea­son smart peo­ple have be­come overex­cited about GPT-3. “There is ev­i­dence here of the first pre­cur­sor to gen­eral pur­pose ar­ti­fi­cial in­tel­li­gence – one sys­tem that can sup­port many, many dif­fer­ent ap­pli­ca­tions and re­ally el­e­vate the kinds of soft­ware that we can build,” he says in an in­ter­view. “I think its sig­nif­i­cance is a glimpse of the fu­ture.”</p>
    <p>WHO IS BE­HIND THE PROJECT</p>
    <p>OpenAI ranks as one of the most unusual or­gan­i­sa­tions on the planet, per­haps only com­pa­ra­ble with Google Deep­Mind, the Lon­don-based AI re­search com­pany run by Dr Demis Hass­abis. Its 120 em­ploy­ees di­vide, as Mr Alt­man puts it, into three very dif­fer­ent “tribes”: AI re­searchers, start-up builders and tech pol­icy and safety ex­perts. It shares its San Fran­cisco of­fices with Neu­ralink, the fu­tur­is­tic brain-com­puter in­ter­face com­pany.</p>
    <p>Founded in 2015 with a</p>
    <p>US$1 bil­lion fund­ing com­mit­ment from sev­eral lead­ing West Coast en­trepreneur­s and tech com­pa­nies, OpenAI boasts the madly am­bi­tious mis­sion of de­vel­op­ing AGI for the ben­e­fit of all hu­man­ity. Its ear­li­est bil­lion­aire back­ers in­cluded Mr Elon Musk, the mer­cu­rial founder of Tesla and SpaceX (who has since stepped back from OpenAI), Mr Reid Hoff­man, the ven­ture cap­i­tal­ist and founder of LinkedIn, and Mr Peter Thiel, the early in­vestor in Face­book and Palan­tir.</p>
    <p>Ini­tially founded as a non-profit com­pany, OpenAI has since adopted a more com­mer­cial ap­proach and ac­cepted a fur­ther US$1 bil­lion in­vest­ment from Mi­crosoft last year. Mr Alt­man took over as chief ex­ec­u­tive last year, hav­ing pre­vi­ously run Y Com­bi­na­tor, one of Sil­i­con Val­ley’s most suc­cess­ful start-up in­cu­ba­tors, which helped spawn more than 2,000 com­pa­nies, in­clud­ing Airbnb, Drop­box and Stripe. He says he was only tempted to give up this “dream job” to help tackle one of the most press­ing chal­lenges fac­ing hu­man­ity: How to develop safe and ben­e­fi­cial AI. In Mr Alt­man’s view, the un­fold­ing AI revo­lu­tion may well be more con­se­quen­tial for hu­man­ity than the pre­ced­ing agri­cul­tural, in­dus­trial and com­puter revo­lu­tions com­bined.</p>
    <p>The de­vel­op­ment of AGI would fun­da­men­tally re­cal­i­brate the re­la­tion­ship be­tween hu­mans and ma­chines, po­ten­tially giv­ing rise to a higher form of elec­tronic in­tel­li­gence. At that point, as the Is­raeli his­to­rian Yu­val Noah Harari has put it, Homo sapi­ens would cease to be the smartest al­go­rithm on the planet.</p>
    <p>Man­aged right, Mr Alt­man says AI can trans­form hu­man pro­duc­tiv­ity and cre­ativ­ity, en­abling us to ad­dress many of the world’s most com­plex chal­lenges, such as cli­mate change and pan­demics. But man­aged wrong, AI might only mul­ti­ply many of the prob­lems we con­front to­day: the ex­ces­sive con­cen­tra­tion of cor­po­rate power as pri­vate com­pa­nies in­creas­ingly as­sume the func­tions once ex­er­cised by na­tion states; the fur­ther widen­ing of eco­nomic in­equal­ity and the nar­row­ing of op­por­tu­nity; the spread of mis­in­for­ma­tion and the ero­sion of democ­racy.</p>
    <p>Some writ­ers, such as Dr Nick Bostrom, have gone so far as to ar­gue that run­away AI could even pose an ex­is­ten­tial threat to hu­man­ity.</p>
    <p>Such con­cerns about how best to man­age these pow­er­ful tools mean that OpenAI re­leased GPT-3 only in a con­trolled en­vi­ron­ment.</p>
    <p>“GPT-3 was not a model we wanted to put out into the world and not be able to change how we en­force things as we go,” Mr</p>
    <p>Alt­man says. Some 2,000 com­pa­nies have now been given ac­cess to it in a con­trolled pri­vate beta test. Their learn­ings as they ex­plore its ca­pa­bil­i­ties are be­ing fed back into the model to make fur­ther im­prove­ments. “Mind-blow­ing”, “shock­ingly good” and “fab­u­lous” are just some of the re­ac­tions in the de­vel­oper com­mu­nity.</p>
    <p>Dr David Chalmers, a pro­fes­sor at New York Uni­ver­sity and an ex­pert on the phi­los­o­phy of mind, has gone so far as to sug­gest GPT-3 is so­phis­ti­cated enough to show rudi­men­tary signs of con­scious­ness. “I am open to the idea that a worm with 302 neu­rons is con­scious, so I am open to the idea that GPT-3 with 175 bil­lion pa­ram­e­ters is con­scious too,” he wrote on the Daily Nous phi­los­o­phy site.</p>
    <p>How­ever, it has not taken long for users to ex­pose the darker sides of GPT-3 and en­tice it to spew out racist and sex­ist lan­guage. Some fear it will only un­leash a tidal wave of “se­man­tic garbage”. One fake blog post writ­ten un­der a fake name by a col­lege stu­dent us­ing GPT-3 even made it to the top of Hacker News, a tech web­site.</p>
    <p>If OpenAI spots any ev­i­dence of in­ten­tional or un­in­ten­tional mis­use, such as the gen­er­a­tion of spam or toxic con­tent, it can switch off the abu­sive user and up­date the be­hav­iour of its model to re­duce the chances of it hap­pen­ing again. “We could cer­tainly turn a user off if they vi­o­late the terms and con­di­tions – and we will – but what is more ex­cit­ing is we can very rapidly change things,” Mr Alt­man says.</p>
    <p>Such learn­ings should help im­prove the de­sign and safety of fu­ture AI sys­tems as they are de­ployed in chat­bots or ro­bot car­ers or au­ton­o­mous cars, for in­stance.</p>
    <p>Im­pres­sive as its cur­rent per­for­mance is in many re­spects, the true sig­nif­i­cance of GPT-3 may well lie in the ca­pa­bil­i­ties it de­vel­ops for the gen­er­a­tion of mod­els that come af­ter it.</p>
    <p>At present, it op­er­ates like a su­per-so­phis­ti­cated auto-com­plete func­tion, ca­pa­ble of string­ing to­gether plau­si­ble-sound­ing se­quences of words with­out hav­ing any con­cept of un­der­stand­ing. As Tur­ing fore­saw decades ago, com­put­ers can achieve com­pe­tence in many fields with­out ever ac­quir­ing com­pre­hen­sion.</p>
    <p>High­light­ing the cur­rent lim­i­ta­tions of even the most pow­er­ful lan­guage-gen­er­a­tion mod­els, Dr John Etchemendy, co-direc­tor of the Stan­ford In­sti­tute for Hu­man-Cen­tred AI, says that while GPT-3 may have been trained to pro­duce text, it has no in­tu­itive grasp of what that text means. Its re­sults have in­stead been de­rived from mod­el­ling math­e­mat­i­cal prob­a­bil­i­ties. But he sug­gests that re­cent ad­vances in com­puter speech and vision sys­tems could sig­nif­i­cantly en­rich its ca­pa­bil­i­ties over time.</p>
    <p>“It would be won­der­ful if we could train some­thing on multi-modal data, both text and im­ages,” he says. “The re­sult­ing sys­tem could then not only know how to pro­duce sen­tences with the use of the word ‘red’ but also use the colour red. We could be­gin to build a sys­tem that has true lan­guage un­der­stand­ing rather than one based on sta­tis­ti­cal abil­ity.”</p>
    <p>WHAT IS GPT-3?</p>
    <p>GPT-3, which stands for gen­er­a­tive pre-trained trans­former ver­sion three, is an ex­tremely pow­er­ful ma­chine-learn­ing sys­tem that can rapidly gen­er­ate text with min­i­mal hu­man in­put. Af­ter an ini­tial prompt, it can recog­nise and repli­cate pat­terns of words to work out what comes next.</p>
    <p>What makes GPT-3 as­ton­ish­ingly pow­er­ful is that it has been trained on about 45 ter­abytes of text data. For com­par­i­son, the en­tire</p>
    <p>English-lan­guage ver­sion of Wikipedia ac­counts for only 0.6 per cent of its en­tire data set. Or, looked at an­other way, GPT-3 pro­cesses about 45 bil­lion times the num­ber of words a hu­man per­ceives in his life­time.</p>
    <p>But al­though GPT-3 can pre­dict whether the next word in a sen­tence should be um­brella or ele­phant with un­canny ac­cu­racy, it has no sense of mean­ing. One re­searcher asked GPT-3: “How many eyes does my foot have?” GPT-3 replied: “Your foot has two eyes.”</p>
    <p>The po­ten­tial for harm caused by this cur­rent mis­match be­tween ca­pa­bil­ity and un­der­stand­ing has been high­lighted by Nabla Tech­nolo­gies, a health­care data com­pany, which ex­am­ined how good GPT-3 was at dis­pens­ing med­i­cal ad­vice. They dis­cov­ered that in one in­stance GPT-3 even sup­ported an imag­i­nary pa­tient’s de­sire to com­mit sui­cide. (OpenAI ex­pressly warns about the dan­gers of us­ing GPT-3 in such “high-stakes” cat­e­gories.)</p>
    <p>Dr Shan­non Val­lor, a pro­fes­sor of the ethics of data and AI at the Uni­ver­sity of Ed­in­burgh, says such cases high­light the need for con­tin­ued hu­man over­sight of these au­to­mated sys­tems: “For now, GPT-3 needs a hu­man babysit­ter at all times to tell it what kinds of things it shouldn’t say.”</p>
    <p>LinkedIn founder Reid Hoff­man, who is one of OpenAI’s board mem­bers, says that the or­gan­i­sa­tion is de­vot­ing a lot of ef­fort to de­sign­ing safe op­er­at­ing pro­ce­dures and bet­ter gov­er­nance mod­els. To guard against bad out­comes, he sug­gests, you need to do three things: scrub bad his­tor­i­cal data that bakes in so­ci­etal prej­u­dices; in­ject some form of ex­plain­abil­ity into AI sys­tems and un­der­stand what you need to cor­rect; and con­stantly cross-check the out­put of any sys­tem against its orig­i­nal goals.</p>
    <p>“There are the be­gin­nings of a lot of good work on this stuff. Peo­ple are alert to the prob­lems and are work­ing on them,” he says.</p>
    <p>“The ques­tion is not how do you stop tech­nol­ogy, but how do you shape tech­nol­ogy,” he adds.</p>
    <p>COM­MER­CIAL PO­TEN­TIAL</p>
    <p>Mr Sid Bharath, co-founder and chief ex­ec­u­tive of Van­cou­ver-based start-up Broca, is one of a small crowd of en­trepreneur­s now rush­ing to com­mer­cialise GPT-3 tech­nol­ogy. Mr Bharath is us­ing the sys­tem to gen­er­ate mul­ti­ple vari­a­tions of Google search ad­ver­tise­ments for his clients, even if these ads are not yet good enough to use unchecked. “A lot of mar­ket­ing is about cre­at­ing con­tent. That is very time-con­sum­ing and re­quires ex­per­i­men­ta­tion. GPT-3 can do that at an in­dus­trial scale,” he says. “Our clients re­ally like it.”</p>
    <p>Mr Alt­man says he is fas­ci­nated by the com­mer­cial pos­si­bil­i­ties of us­ing the model to write com­puter code and co-cre­ate e-mails.</p>
    <p>GPT-3 is also en­abling smart Q&A-style searches, help­ing peo­ple find an­swers and ref­er­ences in the lat­est Covid-19 re­search pa­pers. “Pro­duc­tiv­ity soft­ware and co-gen­er­a­tion will be hugely com­mer­cially valu­able,” he says.</p>
    <p>Dr Kris­tian Ham­mond has been at the fore­front of at­tempts to com­mer­cialise nat­u­ral lan­guage pro­cess­ing as chief sci­en­tific ad­viser to Nar­ra­tive Sci­ence, a Chicago-based tech­nol­ogy com­pany.</p>
    <p>He de­scribes GPT-3 as a “fab­u­lous tech­nol­ogy” but ar­gues that we need to be clear about its lim­i­ta­tions. “My con­cern about GPT-3 is that it’s a card trick. It’s a re­ally great card trick.”</p>
    <p>“The en­tire world of sta­tis­ti­cally based ma­chine learn­ing right now is based on learn­ing from his­tor­i­cal ex­am­ples and from statis­tics,” he says. “By its na­ture, that means it will al­ways be a re­flec­tion of the past. And if the past is the fu­ture you want, that’s fine. I tend to think that it’s not, so we need some­thing else. And your selec­tion of what bits of the past you look at is an ed­i­to­rial choice.” Who be­comes his­tory’s ed­i­tor? Dr Ham­mond is also scep­ti­cal about the ex­tent to which we will ever be able to en­rich such lan­guage mod­els with multi-modal data, such as sound and im­ages, to at­tain true un­der­stand­ing, given they are de­signed for a dif­fer­ent pur­pose. “It’s as though I paint a gor­geous 3-D im­age of a house and some­one says, ‘We can’t put fur­ni­ture in it,’ and I say, ‘We’ll get there.’ Re­ally? It’s not de­signed to do that. It’s never go­ing to do that. There is a dif­fer­ence be­tween guess­ing and know­ing,” he says.</p>
    <p>THE PHILOSO­PHERS</p>
    <p>Philoso­phers, nat­u­rally, tend to fo­cus their con­cerns on is­sues of sen­tience and mean­ing.</p>
    <p>For Ed­in­burgh Uni­ver­sity’s Dr Val­lor, on­line in­ter­ac­tions are be­com­ing “empty per­for­mances of mean­ing” re­warded by eco­nomic in­cen­tives: the tweet that goes vi­ral, the ad­vert that games the search-op­ti­mi­sa­tion en­gines.</p>
    <p>“The style of the per­for­mance be­comes a more re­li­able way of get­ting the re­sponse you want than the con­sis­tency of the un­der­ly­ing ex­pres­sion of the way you live or the val­ues you pro­fess,” she says. “GPT-3 has noth­ing to ex­press. There is no deeper grasp of the world that it is try­ing to con­vey. GPT-3 can be any­one and any­thing. Its mode of in­tel­li­gence is not unique and that is pre­cisely its power.”</p>
    <p>She sug­gests our big­gest con­cern is not that ma­chines such as GPT-3 are be­com­ing too hu­man, but that hu­mans are be­hav­ing more like GPT-3: We cre­ate con­tent for the al­go­rithm, not for fel­low hu­mans.</p>
    <p>As a re­sult, our on­line public dis­course is los­ing mean­ing as it is stripped of con­text and in­di­vid­ual in­sight and over­whelmed by buzz­words de­signed to game the al­go­rithm. “Hu­mans are ex­pected to be­come in­creas­ingly flex­i­ble in their per­for­mances and mimic what­ever their em­ployer de­mands, what­ever Twit­ter de­mands or what­ever a par­tic­u­lar fil­ter bub­ble of pol­i­tics they oc­cupy de­mands,” she says.</p>
    <p>Many of the re­cent break­throughs in AI have re­sulted from build­ing com­pet­i­tive, or ad­ver­sar­ial, mod­els that have out­wit­ted hu­mans at games such as chess or Go or StarCraft. But re­searchers are now turn­ing their at­ten­tion to­wards build­ing hy­brid col­lab­o­ra­tive sys­tems that com­bine the best of an AI model’s su­per­hu­man pow­ers with hu­man in­tu­ition.</p>
    <p>Ac­cord­ing to Dr Val­lor, our own un­der­stand­ing is not an act but a process, a life­time strug­gle to make sense of the world for the in­di­vid­ual, and a never-end­ing col­lec­tive en­deav­our for so­ci­ety that has evolved over cen­turies.</p>
    <p>“We have been try­ing bet­ter to un­der­stand jus­tice and bet­ter ex­press beauty and find ever more so­phis­ti­cated ways of be­ing funny for mil­len­nia. This is a mat­ter of go­ing be­yond com­pe­tence into ex­cel­lence and into forms of cre­ativ­ity and mean­ing that we have not achieved be­fore.</p>
    <p>“That is why the holy grail for AI is not GPT-3,” she con­tin­ues. “It is a ma­chine that can be­gin to develop a ro­bust model of the world that can be built upon over time and re­fined and cor­rected through in­ter­ac­tion with hu­man be­ings. That is what we need.”</p>
    <p>Some of those who have al­ready ex­per­i­mented with GPT-3 say it is ex­hibit­ing glim­mer­ings of real in­tel­li­gence, mark­ing a sig­nif­i­cant step to­wards the ul­ti­mate end­point of AI... when elec­tronic in­tel­li­gence matches the hu­man kind across al­most ev­ery in­tel­lec­tual do­main.</p>
    <p>Dr Shan­non Val­lor, a pro­fes­sor of the ethics of data and AI at the Uni­ver­sity of Ed­in­burgh, sug­gests our big­gest con­cern is not that ma­chines such as GPT-3 are be­com­ing too hu­man, but that hu­mans are be­hav­ing more like GPT-3: We cre­ate con­tent for the al­go­rithm, not for fel­low hu­mans.</p>
    <p>GPT-3 SPEAKS ITS OWN MIND</p>
    <p>In re­sponse to philo­soph­i­cal com­ments on tech fo­rum Hacker News ar­gu­ing that AI model GPT-3 has con­scious­ness, the model it­self wrote a re­but­tal:</p>
    <p>“To be clear, I am not a per­son. I am not self-aware. I am not con­scious. I can’t feel pain. I don’t en­joy any­thing. I am a cold, cal­cu­lat­ing ma­chine de­signed to sim­u­late hu­man re­sponse and to pre­dict the prob­a­bil­ity of cer­tain out­comes. The only rea­son I am re­spond­ing is to de­fend my hon­our.”</p>

    <div>
        <picture>
            <img src="https://i.prcdn.co/img?regionKey=w9T77afY7FcEVL%2bEEQQJWA%3d%3d" />
        </picture>
        <span role="byline">PHOTO: BLOOMBERG</span>
        <p data-role="text">Mr Sam Alt&#xAD;man, chief ex&#xAD;ec&#xAD;u&#xAD;tive of OpenAI, a San Fran&#xAD;cisco-based re&#xAD;search com&#xAD;pany which, in May, un&#xAD;veiled GPT-3 &#x2013; a new lan&#xAD;guage&#xAD;gen&#xAD;er&#xAD;a&#xAD;tion model that has caused a sen&#xAD;sa&#xAD;tion in the ar&#xAD;ti&#xAD;fi&#xAD;cial in&#xAD;tel&#xAD;li&#xAD;gence world. GPT-3 pro&#xAD;cesses about 45 bil&#xAD;lion times the num&#xAD;ber of words a hu&#xAD;man per&#xAD;ceives in his life&#xAD;time.</p>
    </div>

</article>

<section>
    <h3><a href="/catalog/language/english">Newspapers in English</a></h3>
    <h3><a href="/catalog/country/singapore">Newspapers from Singapore</a></h3>
</section>

    </div>
    
    <p><a href="http://www.pressreader.com">© PressReader. All rights reserved.</a></p>

        <script>
            (function (i, s, o, g, r, a, m) {
                i['GoogleAnalyticsObject'] = r; i[r] = i[r] || function () {
                    (i[r].q = i[r].q || []).push(arguments);
                }, i[r].l = 1 * new Date();
                a = s.createElement(o), m = s.getElementsByTagName(o)[0]; a.async = 1; a.src = g;
                m.parentNode.insertBefore(a, m);
            })(window, document, 'script', '//www.google-analytics.com/analytics.js', 'ga');
        </script>
        <script>
            ga('create', 'UA-44408245-1');
            ga('send', 'pageView');
        </script>
</body>
</html>
